{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "375391bf-b8c6-4626-a7c9-8c22e4f051bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangModelWithDense(nn.Module):\n",
    "    def __init__(self, lang_model, vocab_size, input_size, hidden_size, fine_tune):\n",
    "        super(LangModelWithDense, self).__init__()\n",
    "        self.lang_model = lang_model\n",
    "        self.lang_model.resize_token_embeddings(vocab_size + 2 if fine_tune else vocab_size)\n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.8 if fine_tune else 0.1)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.8 if fine_tune else 0.1)\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.fine_tune = fine_tune\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        if self.fine_tune:\n",
    "            embeddings = self.lang_model(x, attention_mask=mask)[0]\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                self.lang_model.eval()\n",
    "                embeddings = self.lang_model(x, attention_mask=mask)[0]\n",
    "\n",
    "        if \"xlnet\" in str(type(self.lang_model)):\n",
    "            embeddings = embeddings[:, 0, :]\n",
    "        else:\n",
    "            embeddings = torch.mean(embeddings, dim=1)\n",
    "\n",
    "        output = self.dropout1(F.gelu(self.linear1(embeddings)))\n",
    "        output = self.dropout2(F.gelu(self.linear2(output)))\n",
    "        output = torch.sigmoid(self.linear3(output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5b340bd-8a15-4b4b-9cbf-227d6debbda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, tokenizer, fine_tune):\n",
    "    df[0] = df[0].apply(lambda x: re.sub(\"\\[ ?link ?\\][a-z]?( \\( [a-z] \\))?\", \"<link>\" if fine_tune else tokenizer.unk_token, x))\n",
    "\n",
    "    df[0] = df[0].apply(lambda x: re.sub(r\" ?https?:.+(\\)|/|(\\.pdf)|(\\.PDF)|(\\.html)|#| - U |aspx?|-[a-zA-z0-9]+|\\.htm|\\?.+)\", \"\", x))\n",
    "    df[0] = df[0].apply(lambda x: re.sub(r\"www.+?( |\\))\", \"\", x))\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\".  .\", \".\").replace(\". .\", \".\").replace(\", .\", \".\"))\n",
    "\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\"“ \", \"\\\"\").replace(\" ”\", \"\\\"\").replace(\"’\", \"'\").replace(\"‘\", \"'\").replace(\",\", \",\").replace(\"⋅\", \"*\"))\n",
    "\n",
    "    df[0] = df[0].apply(lambda x: re.sub(r\" size 12.+}\", \"\", x))\n",
    "    df[0] = df[0].apply(lambda x: re.sub(r\"5 \\\" MeV/\\\"c.+}\", \"\", x))\n",
    "    df[0] = df[0].apply(lambda x: re.sub(r\" } { }\", \"\", x))\n",
    "\n",
    "    df[0] = df[0].apply(lambda x: re.sub(r\"[^\\s]+(\\+|=|Δ|\\*){1}[^\\s]+\", \"<equation>\" if fine_tune else tokenizer.unk_token, x))\n",
    "\n",
    "    df[0] = df[0].apply(lambda x: re.sub(r\"^ (\\d+ . )?\", \"\", x))\n",
    "\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\"do n't\", \"don't\").replace(\"Do n't\", \"Don't\"))\n",
    "\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\" .\", \".\"))\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\" ,\", \",\"))\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\" ?\", \"?\"))\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\" - \", \"-\"))\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\"( \", \"(\"))\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\" )\", \")\"))\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\" & \", \"&\"))\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\" ;\", \";\"))\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\" '\", \"'\"))\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\" :\", \":\"))\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\" $\", \"$\"))\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\" %\", \"%\"))\n",
    "    df[0] = df[0].apply(lambda x: re.sub(r\"(_ )+\", \"\", x))\n",
    "    df[0] = df[0].apply(lambda x: x.replace(\",\\\"\", \"\\\"\"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef194462-83f3-4579-877c-475eec3902d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_sentences(df, tokenizer, fine_tune, device):\n",
    "    mask = []\n",
    "\n",
    "    df = clean_data(df, tokenizer, fine_tune)[0]\n",
    "\n",
    "    X = df.values\n",
    "\n",
    "    tokens = []\n",
    "    for i in range(X.shape[0]):\n",
    "        t = torch.tensor(tokenizer.encode(X[i], add_special_tokens=True))\n",
    "        tokens.append(t)\n",
    "        mask.append(torch.ones_like(t))\n",
    "\n",
    "    X = torch.nn.utils.rnn.pad_sequence(tokens, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n",
    "    mask = pad_sequence(mask, batch_first=True, padding_value=0).to(device)\n",
    "\n",
    "    # X_list.append(torch.tensor(X))\n",
    "\n",
    "    return X, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fbb3db7-7d40-4aa9-aa62-6ecce3b7f500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "\n",
    "lang_model_name = 'roberta-base'\n",
    "device = torch.device('cuda')\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(lang_model_name)\n",
    "lang_model = RobertaModel.from_pretrained(lang_model_name)\n",
    "\n",
    "model = LangModelWithDense(lang_model, len(tokenizer), 768, 512, True).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"./fine-tuned/model-finetuned_new.pth\"))\n",
    "\n",
    "a = [[\"This NER work is just a small part of the work we are doing at the UN Emerging Technology Lab (ETL) to accelerate the UNs mission.\", None],\n",
    "    [\"A one time offline processing creates a mapping from the sets of context independent signatures harvested from BERTs vocabulary to a single descriptor/label.\", None],\n",
    "    [\"BERTs cased vocabulary is a mixture of common nouns, proper nouns, subwords and symbols. The resulting set of 21,418 terms a mixture of common nouns and proper nouns serve as descriptors characterizing an entity type.\", None],\n",
    "    [\"A tokenizer is responsible for preparing the text inputs as inputs to the transformer model\", None],\n",
    "    [\"NERDA is open-sourced and available on the Python Package Index (PyPI). It can be installed with:\", None],\n",
    "    [\"We will use the English CoNLL-2003 data set with NER annotations for training and validation of our model.\", None],\n",
    "    [\"Recurrent Neural Networks (RNN) are designed to work with sequential data. Sequential data(can be time-series) can be in form of text, audio, video etc.\", None],\n",
    "    [\"RNN uses the previous information in the sequence to produce the current output. To understand this better Im taking an example sentence.\", None],\n",
    "    [\"The workflow of GRU is same as RNN but the difference is in the operations inside the GRU unit. Lets see the architecture of it.\", None],\n",
    "    [\"In the LSTM layer, I used 5 neurons and it is the first layer (hidden layer) of the neural network, so the input_shape is the shape of the input which we will pass.\", None],\n",
    "    [\"A Recurrent Neural Network is a network with a loop. It processes information sequentially and the output from every time step is fed back to the network which acts as a sort of memory.\", None],\n",
    "    [\"the concatenated vector is passed through a fully connected layer to get a new vector. This vector is passed through the softmax activation.\", None]]\n",
    "\n",
    "test_df = pd.DataFrame(a)\n",
    "tokens, masks = process_test_sentences(test_df, tokenizer, True, device)\n",
    "\n",
    "output = model.forward(tokens, masks)\n",
    "pred = torch.tensor([0 if x < 0.5 else 1 for x in output])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a38ff-7625-47d9-83a0-ded7ab8b20a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
